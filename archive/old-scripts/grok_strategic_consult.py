#!/usr/bin/env python3
"""Consult Grok-2 for strategic analysis of our Jones polynomial project."""

import json
import subprocess
import os

prompt = """You are a strategic advisor for mathematical research projects. Analyze this situation with maximum depth and provide actionable recommendations.

CONTEXT:
We're attempting to systematically verify the Jones Unknotting Conjecture using Aristotle AI + Lean 4 formal verification.

CURRENT STATUS:
- ‚úÖ 18/1,126 knots verified (1.6%)
- ‚úÖ First batch: 10 knots in 3-4 minutes (10-40x faster than expected!)
- ‚úÖ >99% confidence validation (4 independent methods)
- ‚úÖ Methodology proven viable
- üìä Target: All 1,126 non-alternating knots up to 12 crossings

KEY DISCOVERY:
Speed is INCREDIBLE - we could finish ALL 1,126 knots in DAYS not WEEKS at this rate!

CRITICAL QUESTIONS:

1. PUBLICATION STRATEGY:
   - Where should we publish? (Experimental Mathematics, ITP, CPP, Nature Comms?)
   - What makes this publishable vs. just "verification work"?
   - How to position: First formal verification? AI methodology? Statistical analysis?

2. WHAT MAKES THIS EPIC:
   - What additional analysis would maximize impact?
   - Should we do pattern analysis? Machine learning? Cross-correlations?
   - What would knot theorists find interesting beyond raw verification?

3. COMPLETION STRATEGY:
   - Conservative: Finish each crossing systematically (1-2 weeks)
   - Aggressive: Parallel batching, finish in 3-7 days
   - Build automation: Queue manager running 24/7, finish in 15-20 hours
   Which approach and why?

4. COLLABORATION:
   - Who should we contact? (Sazdanovic for TDA? KnotInfo maintainers? Tao?)
   - When to reach out? (Now vs after completion?)
   - What to offer collaborators?

5. VALIDATION:
   - Is >99% confidence sufficient for publication?
   - What additional validation should we do?
   - Grok analysis, KnotInfo cross-check, deep polynomial comparison - enough?

6. RISKS:
   - What could go wrong at scale?
   - What if we find a counterexample (Jones = 1)?
   - What if computational speed degrades at 11-12 crossings?

7. IMPACT MAXIMIZATION:
   - Beyond verification, what insights can we extract?
   - How to frame this as NOVEL not just "checking known things"?
   - What would make this Nature-worthy vs. just domain journal?

8. TIMELINE REALISM:
   - Given the speed, should we push for EVERYTHING this week?
   - Or pace ourselves for better validation?
   - Trade-offs between speed and thoroughness?

BE BRUTALLY HONEST. Challenge assumptions. Identify blind spots. What are we missing?

Give specific, actionable recommendations with prioritization."""

request = {
    "messages": [{"role": "user", "content": prompt}],
    "model": "grok-2-latest",
    "temperature": 0.7,  # Higher for strategic creativity
    "max_tokens": 4000
}

print("ü§ñ Consulting Grok-2 for Strategic Analysis...")
print("=" * 70)
print()

with open('/tmp/grok_strategic_request.json', 'w') as f:
    json.dump(request, f, indent=2)

result = subprocess.run(
    ['curl', '-X', 'POST', 'https://api.x.ai/v1/chat/completions',
     '-H', f'Authorization: Bearer {os.environ.get("GROK_API_KEY")}',
     '-H', 'Content-Type: application/json',
     '-d', '@/tmp/grok_strategic_request.json'],
    capture_output=True, text=True, timeout=300
)

if result.returncode == 0:
    response = json.loads(result.stdout)
    analysis = response['choices'][0]['message']['content']

    print("GROK-2 STRATEGIC ANALYSIS")
    print("=" * 70)
    print()
    print(analysis)
    print()

    with open('/Users/patrickkavanagh/math/GROK_STRATEGIC_ANALYSIS.md', 'w') as f:
        f.write("# Grok-2 Strategic Analysis\n\n")
        f.write("**Date**: December 12, 2025\n")
        f.write("**Model**: grok-2-latest (temperature 0.7)\n\n")
        f.write("---\n\n")
        f.write(analysis)

    print("‚úÖ Analysis saved to GROK_STRATEGIC_ANALYSIS.md")
else:
    print(f"‚ùå Error: {result.stderr}")
